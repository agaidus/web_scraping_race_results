{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Race Results Part 1: Web Scraping\n",
    "\n",
    "I enjoyed this analysis a lot, mainly because I found the data really fun to play with. It started as a way to learn how to scrape HTML data from a website, but then I decided that it would also be useful to dig a bit deeper intothe data and do a bit more analyzing and visualizing. It also provided a good opportunity to make use of various scientific Python libraries for data wrangling, data visualization, and statistical modeling.\n",
    "\n",
    "The data that I use are the [2015](http://www.empirerunners.com/results/empireopen/eo15.html) and [2016](http://www.empirerunners.com/results/empireopen/eo16.html) results from the Empire Open Cross Country Meet - a 3.43 mile race that my running club hosts every year in Santa Rosa. The data has all the typical information you'd expect to find in race results - team, age, gender, time, etc. \n",
    "\n",
    "The analysis can be broken up into the following steps:\n",
    "\n",
    "1. **Web scraping and Data Cleaning** - use the ```BeautifulSoup``` Python library to scrape HTML data from a website\n",
    "\n",
    "2. **Data Wrangling** - use ```Pandas``` grouping operations to reshape data into a more usable form and generate stats that compare team performances across years\n",
    "\n",
    "3. **Data visualization** - use ```Matplotlib``` and its wrapper ```seaborn``` to generate plots that help visualize these comparisons\n",
    "\n",
    "4. **Modeling** - use ```Statsmodels``` to built a basic linear regression model that can be used to control for age and gender when comparing times.\n",
    "\n",
    "In this initial post, I will focus on the first aspect - obtaining and cleaning raw data from the web.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import necessary Python modules\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML Data with ```BeautifulSoup```\n",
    "\n",
    "Beautiful Soup is a Python library for parsing data out of HTML and XML files. It constructs a parse tree based on the HTML tags and provides a really easy way to navigate, search, and extract data based on these tags. \n",
    "\n",
    "Below, I specify the URL paths for each of the two years of data. For now, I'm going to parse only the 2015 results, just as a way of demonstrating how to extract the data from HTML. I use ```urllib2``` to read in the website URL as a string of HTML code, and then I pass this string to ```BeautifulSoup``` to be parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "years = [2015, 2016]\n",
    "urls = ['http://www.empirerunners.com/results/empireopen/eo{}.html'.format(str(y)[2:]) for y in years]                   \n",
    "                                                                           \n",
    "html = urllib2.urlopen(urls[0]).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a screenshot of the website that shows the data that I'm interesting in parsing. As you can see, there is a table structure that contains the race results.\n",
    "\n",
    "![title](website_screenshot.png)\n",
    "\n",
    "\n",
    "Now, that I have passed the HTML code to ```BeautifulSoup```, I can call for specific tag objects to extract relevant data. I search for and print the 'title' tag below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Phil Widener Empire Open 2015</title>\n"
     ]
    }
   ],
   "source": [
    "print soup.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what I'm most interested in is the table that contains the results. First I use \"find_all\" to look for all elements with the \"table\" tag and confirm that there is only one table on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print len(soup.find_all('table'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I go ahead and extract the HTML code corresponding to the table and print it (part of it), using the ```prettify``` method which gives a better visual representation of the tree structure. The important things to note are the ```<td>``` and ```<tr>``` tags. ```<tr>``` is used to identify each row within a table and ```<td>``` identifies each cell within each row. As you can see the ```<td>``` tag always is nested within the ```<tr>``` tag. Knowing this structure allows us to parse apart the data elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"racetable\">\n",
      " <tr>\n",
      "  <td class=\"h01\" colspan=\"9\">\n",
      "   <h3>\n",
      "    Women's Race\n",
      "   </h3>\n",
      "  </td>\n",
      " </tr>\n",
      " <tr>\n",
      "  <td class=\"h11\">\n",
      "   Place\n",
      "  </td>\n",
      "  <td class=\"h12\">\n",
      "   Name\n",
      "  </td>\n",
      "  <td class=\"h12\">\n",
      "   Team\n",
      "  </td>\n",
      "  <td class=\"h11\">\n",
      "   Bib No\n",
      "  </td>\n",
      "  <td class=\"h11\">\n",
      "   Age\n",
      "  </td>\n",
      "  <td class=\"h11\">\n",
      "   Gender\n",
      "  </td>\n",
      "  <td class=\"h11\">\n",
      "   Age Group\n",
      "  </td>\n",
      "  <td class=\"h11\">\n",
      "   Total Time\n",
      "  </td>\n",
      "  <td class=\"h11\">\n",
      "   Pace\n",
      "  </td>\n",
      " </tr>\n",
      " <tr>\n",
      "  <td class=\"d01\">\n",
      "   1\n",
      "  </td>\n",
      "  <td class=\"d02\">\n",
      "   Alexandra Sciocchetti\n",
      "  </td>\n",
      "  <td class=\"d02\">\n",
      "   UNATTACHED\n",
      "  </td>\n",
      "  <td class=\"d01\">\n",
      "   189\n",
      "  </td>\n",
      "  <td class=\"d01\">\n",
      "   20\n",
      "  </td>\n",
      "  <td class=\"d01\">\n",
      "   F\n",
      "  </td>\n",
      "  <td class=\"d01\">\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "table = soup.find('table')\n",
    "print table.prettify()[:750]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a nested list comprehension along with the BeautifulSoup search tags, I can extract the data from the table in one line of code! I find all the table rows by searching for the ```<tr>``` tag with the command ```table.find_all('tr')```. I find the cell values for each cell within each row by searching for the ```<td>``` tag using ```row.find_all('td')```. I extract only the text identified by these tags and use the ```strip()``` method to get rid of white space. The result is a nested list of all of the table data elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u\"Women's Race\"],\n",
       " [u'Place',\n",
       "  u'Name',\n",
       "  u'Team',\n",
       "  u'Bib No',\n",
       "  u'Age',\n",
       "  u'Gender',\n",
       "  u'Age Group',\n",
       "  u'Total Time',\n",
       "  u'Pace'],\n",
       " [u'1',\n",
       "  u'Alexandra Sciocchetti',\n",
       "  u'UNATTACHED',\n",
       "  u'189',\n",
       "  u'20',\n",
       "  u'F',\n",
       "  u'1/48 13-39',\n",
       "  u'20:20',\n",
       "  u'5:55/M'],\n",
       " [u'2',\n",
       "  u'Tamma Carleton',\n",
       "  u'Strawberry Canyon TC',\n",
       "  u'114',\n",
       "  u'27',\n",
       "  u'F',\n",
       "  u'2/48 13-39',\n",
       "  u'20:23',\n",
       "  u'5:56/M']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = [[cell_value.text.strip() for cell_value in row.find_all('td')] for row in table.find_all('tr')]\n",
    "raw_data[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are a few aspects of this table that will need to be cleaned before we are ready to use the data. First, you'll notice that the first element of the list is the table title. It will also appear later in the data for the \"Men's Race\". Then, the second element of the list is the table header. For some reason, this header also re-appears as a row in the data every 100 rows. Nonetheless, all of this is easy to filter out. Below I clean the data and then read it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Place                   Name                    Team Bib No Age Gender  \\\n",
      "0     1  Alexandra Sciocchetti              UNATTACHED    189  20      F   \n",
      "1     2         Tamma Carleton    Strawberry Canyon TC    114  27      F   \n",
      "2     3            Julia Sizek    Strawberry Canyon TC    191  24      F   \n",
      "3     4         Michele Palmer      Impala Racing Team    174  33      F   \n",
      "4     5    Stephanie MacKenzie  West Valley Track Club    160  39      F   \n",
      "\n",
      "    Age Group Total Time    Pace  \n",
      "0  1/48 13-39      20:20  5:55/M  \n",
      "1  2/48 13-39      20:23  5:56/M  \n",
      "2  3/48 13-39      20:26  5:57/M  \n",
      "3  4/48 13-39      20:39  6:01/M  \n",
      "4  5/48 13-39      20:43  6:02/M  \n"
     ]
    }
   ],
   "source": [
    "#Identify the header as the first row that has a length of 9 (excludes the title)\n",
    "header = [row for row in raw_data if len(row) == 9][0]\n",
    "\n",
    "#Loop through each row and only append those rows that are not table titles (length<>9) or table headers\n",
    "cleaned_data =[]\n",
    "for row in raw_data:\n",
    "    if row <> header and len(row) == 9:\n",
    "        cleaned_data.append(row)\n",
    "        \n",
    "print pd.DataFrame(data = cleaned_data, columns = header).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I understand the structure of the data, I am ready to loop through each of the two years; process the data and then combine them into a dataframe. To make things easier, I first write a function that takes the data from ```BeautifulSoup``` and cleans it, taking the steps shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_results_table(url):\n",
    "    html = urllib2.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    table = soup.find('table')\n",
    "    raw_data = [[cell_value.text.strip() for cell_value in row.find_all('td')] for row in table.find_all('tr')]\n",
    "    \n",
    "    header = [row for row in raw_data if len(row) == 9][0]\n",
    "\n",
    "    cleaned_data =[]\n",
    "    for row in raw_data:\n",
    "        if row <> header and len(row) == 9:\n",
    "            cleaned_data.append(row)\n",
    "\n",
    "    return pd.DataFrame(data = cleaned_data, columns = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for year, url in zip(years, urls):\n",
    "    d = parse_results_table(url)\n",
    "    d['Year']=year\n",
    "    df = df.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are just a few small cleaning steps, and then I'm ready to analyze the data. A key field in this analysis will be the \"Total Time\" field, which is currently represented as a string. I write a function to convert this string to a ```timedelta``` object and then also calculate the time in minutes as a ```float``` (this will be useful for some of the plotting functions I do later). Lastly, I sort the values by Time and export the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Place              Name                    Team  Bib No  Age Gender  \\\n",
      "0      1      Isaac Chavez     Aggies Running Club     414   24      M   \n",
      "1      2  Malcolm Richards  West Valley Track Club     306   32      M   \n",
      "2      1    Trevor Halsted              UNATTACHED     701   23      M   \n",
      "3      2  Malcolm Richards  West Valley Track Club     623   33      M   \n",
      "4      3  Alfonso Cisneros     Aggies Running Club     413   23      M   \n",
      "\n",
      "    Age Group Total Time    Pace  Year     Time    Minutes  \n",
      "0  1/52 16-99      16:51  4:54/M  2015 00:16:51  16.850000  \n",
      "1  2/52 16-99      16:56  4:56/M  2015 00:16:56  16.933333  \n",
      "2     1 16-99      16:56  4:56/M  2016 00:16:56  16.933333  \n",
      "3     2 16-99      17:05  4:59/M  2016 00:17:05  17.083333  \n",
      "4  3/52 16-99      17:07  4:59/M  2015 00:17:07  17.116667  \n"
     ]
    }
   ],
   "source": [
    "def convert_date_string_to_datetime(time_string):\n",
    "    t=datetime.strptime(time_string, \"%M:%S\")\n",
    "    delta = timedelta(minutes=t.minute, seconds=t.second)\n",
    "    return delta\n",
    "\n",
    "df['Time']= df['Total Time'].apply(convert_date_string_to_datetime)\n",
    "df['Minutes'] = df['Time'].dt.seconds/60\n",
    "df[['Place' ,'Age', 'Bib No']] = df[['Place' ,'Age', 'Bib No']].astype(int)\n",
    "df = df.sort_values(by = 'Time').reset_index(drop=True)\n",
    "\n",
    "df.to_pickle('cleaned_race_results.p')\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Now I have a dataset that is cleaned and ready to analyze."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
